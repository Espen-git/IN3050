{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with some random points\n",
    "points=[\n",
    "    (1,10),\n",
    "    (2,5),\n",
    "    (3,7),\n",
    "    (4,8),\n",
    "    (4,4),\n",
    "    (6,3),\n",
    "    (7,2),\n",
    "    (8,5),\n",
    "    (8,1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the points in a NumPy table.\n",
    "data = np.array(points)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data.\n",
    "plt.scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the weights to (0,0) for a baseline\n",
    "weights = np.array([0,0])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line over the interval [0,10] if you know w0 and w1 for the line.\n",
    "def plotline(w0=weights[0], w1=weights[1], color='r'):\n",
    "    x0  = 0\n",
    "    y0  = w0\n",
    "    x10 = 10\n",
    "    y10 = w0 + w1 * 10\n",
    "    plt.plot([x0,x10], [y0, y10], color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0],data[:,1])\n",
    "plotline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input data from their targets.\n",
    "# All but the last column are input data.\n",
    "X = data[:, :1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last column contains the target values.\n",
    "Tar = data[:, 1:]\n",
    "Tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the weights into a column vector, a (n+1)x1 matrix\n",
    "# n is the number of features for a data point.\n",
    "W = weights.reshape(-1,1)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construt bias as 1-s.\n",
    "Bias = np.ones((X.shape[0], 1))\n",
    "Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the bias into position 0 of the input data.\n",
    "# Observe the argument axis=1\n",
    "Xb = np.concatenate((Bias, X), axis=1)\n",
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remeber earlier weights when we update.\n",
    "old_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate\n",
    "eta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat what the data and baseline looks like.\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "plotline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Xb @ W\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Y - Tar\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grad = Xb.T @ D\n",
    "Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights.append((W[0,0], W[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W - eta * Grad\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result of updating the weights.\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate\n",
    "Return to the headline *Gradient descent* and run again the steps from there to here and see the change.\n",
    "\n",
    "Repeat 3-4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (20):\n",
    "    Y = Xb @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xb.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (200):\n",
    "    Y = Xb @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xb.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    Y = Xb @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xb.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for i in range(0,2000,200):\n",
    "    ws = old_weights[i]\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "You may experiment with larger learning rates, eg., 0.005 or 0.01.\n",
    "\n",
    "You wil probably want to stop the training at some point when the results are satisfactory.\n",
    "More on that in the weekly sets 7 and 8 and Mandatory 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
