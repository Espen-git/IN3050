{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backprop session - Agenda:** \n",
    "- ## Recall what makes up a neural network \n",
    "\n",
    "- ## Review the forward pass, sending data through the network to produce an output \n",
    "\n",
    "- ## Discuss some intuition for the backpropagation algorithm \n",
    "\n",
    "- ## Do a pen-and-paper example of a single round of backpropagation. \n",
    "\n",
    "- ## Will **not** cover the details of calculating the gradient. (The equations are included in this notebook for completeness). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The neural network** \n",
    "\n",
    "### Will consider a neural network for **regression**, with target values in $\\mathbb{R}$ (that is, targets are real values in one dimension).\n",
    "### Neural networks can also be used for calssification, and target values can have multiple classes (or dimensions in regression).  \n",
    "\n",
    "## **Termnology** (This may vary from author to author):\n",
    "### $V$: The set of weights connecting the input layer to the hidden neurons. $V_{i, j}$ connects input neuron i to hidden neuron j\n",
    "\n",
    "### $W$: Connecting the hidden neurons to the output neuron(s): $W_{i, j}$ connects hidden neuron $i$ to output neuron $j$. (in our case, we only have $j = 1$ since output is in one dimension). \n",
    "\n",
    "### $\\mathbf{x} = (x_1, x_2)$ is a single data example with two features. \n",
    "\n",
    "### $z_j$ is the total signal into hidden neuron j. <font color = \"red\"> (Beware: in the lecture slides, $z_j$ is the input into **output neuron j**. In the slided, our $z_j$ is called $h_j$).  </font>\n",
    "\n",
    "### Activation function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "### $a_j$ is the **activated** signal out of hidden neuron $j$. $a_j = \\sigma(z_j)$\n",
    "\n",
    "### $o$ is the total signal into the output neuron\n",
    "\n",
    "### $y$ is the total signal out of the output neuron. \n",
    "\n",
    "### $t$ is the target value for the datapoint $\\mathbf{x}$\n",
    "\n",
    "### Loss function: Squared error -  $\\mathcal{L}(y, t) = (y-t)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural network](img/neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The forward pass:** \n",
    "### The signal into the hidden neurons is the sum of input value times weights connecting into that neuron.\n",
    "### Here's an example of the **signal into the first hidden neuron.**\n",
    "\n",
    "### The accumulated signal **into the first hidden neuron** is $z_1$\n",
    "![image.png](img/into_hidden_1.png)\n",
    "\n",
    "### $z_1 = -1 \\cdot V_{0,1} + x_1 \\cdot V_{1,1} + x_2 \\cdot V_{2,1}  $. Likewise, $z_2 = -1 \\cdot V_{0,2} + x_1 \\cdot V_{1,2} + x_2 \\cdot V_{2,2}  $ , $z_3 = -1 \\cdot V_{0,3} + x_1 \\cdot V_{1,3} + x_2 \\cdot V_{2,3}  $ \n",
    "\n",
    "### Before the signal gets passed fom the hidden neurons, the **activation function** is applied. \n",
    "### The **activated signals out of the hidden neurons are** $$(a_1, a_2, a_2) = (\\sigma(z_1), \\sigma(z_2), \\sigma(z_3))$$ \n",
    "\n",
    "\n",
    "### The signal **into the output neuron** is the sum of the activated signals $\\mathbf{a}$ and the bias $-1$ multiplied with the weights connecting to the output neuron: \n",
    "\n",
    "### $$y = -1 \\cdot W_{0,1} + a_1 \\cdot W_{1,1} + a_2 \\cdot W_{2,1} +  a_3 \\cdot W_{3,1} .$$ \n",
    "\n",
    "### Since this is a regression network, we do not apply an activation function to the output, so $o = y$. The output value $y$ is the predicted value for the input $(x_1, x_2)$\n",
    "\n",
    "### Let's make an example. We initialize the network with these weights: \n",
    "\n",
    "![image.png](img/with_weights.png)\n",
    "\n",
    "### Weights from bias neuron: \n",
    "$V_{0, 1} = 1$,\n",
    "\n",
    "$V_{0, 2} = 0$,\n",
    "\n",
    "$V_{0, 3} = -1$.\n",
    "\n",
    "### Weights from feature 1: \n",
    "$V_{1, 1} = 1$,\n",
    "\n",
    "$V_{1, 2} = 2$,\n",
    "\n",
    "$V_{1, 3} = 0$.\n",
    "\n",
    "### Weights from feature 1: \n",
    "$V_{2, 1} = 0$,\n",
    "\n",
    "$V_{2, 2} = -2$,\n",
    "\n",
    "$V_{2, 3} = 1$.\n",
    "\n",
    "### Weights from the hidden neurons: \n",
    "\n",
    "$W_{0, 1} = 1$,\n",
    "\n",
    "$W_{1, 1} = 2$,\n",
    "\n",
    "$W_{2, 1} = 3$,\n",
    "\n",
    "$W_{2, 1} = -1$.\n",
    "\n",
    "### Input data: $\\mathbf{x} = (1, 2)$\n",
    "\n",
    "### Target value: $t = 5$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **Using the wieghts and inputs from above:** \n",
    "### \\begin{align} z_1 &= -1 \\cdot 1 + 1 \\cdot 1 + 2 \\cdot 0 = 0 \\\\\n",
    "z_2 &= -1 \\cdot 0 + 1 \\cdot 2 + 2 \\cdot -2  = -2 \\\\ \n",
    "z_3 &= -1 \\cdot -1 + 1 \\cdot 0 + 2 \\cdot 1  = 3 \\\\ \n",
    "\\end{align}\n",
    "## Activations out of the hidden neurons: \n",
    "### \\begin{align} a_1 = \\sigma(z_1) = \\sigma(0) &= 0.5 \\\\\n",
    "a_2 = \\sigma(z_2) = \\sigma(-2) & \\approx 0.1192\\\\ \n",
    "a_3 = \\sigma(z_3)  = \\sigma(z_3) &\\approx 0.952 \n",
    "\\end{align}\n",
    "\n",
    "## Signal into output neuron: \n",
    "### $$ o =  -1 \\cdot 1 + 0.5 \\cdot 2 + 0.1192 \\cdot 3 +  0.952 \\cdot -1  \\approx -0.5944 $$\n",
    "\n",
    "## Since this is a network for regression, we don't apply any activation function to the output. The output from the network is \n",
    "### $$ y \\approx -0.5944 $$\n",
    "\n",
    "![image.png](img/img_1.png)\n",
    "\n",
    "### (Above calculations can be done efficiently with matrix multiplication and vector operations in python. CF lecture slides, or ask in group session!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br> \n",
    "\n",
    "# **Back propagation - some intuition**\n",
    "\n",
    "\n",
    "### Recall that the target was $5$ while the prediction was $-0.59$. This gives an MSE of $(-0.59 - 5)^2 \\approx 31.25$.\n",
    "### The goal is to use **gradient descent** to find weights $V, W$ that minimize this error, by stepping in the opposite direction of the gradient.\n",
    "\n",
    "# $$\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla \\mathcal{L}( t, y)$$\n",
    "\n",
    "### If we know the gradient of $\\mathcal{L}$, we know how to update the weights to reduce the mistake we made! \n",
    "\n",
    "<br><br><br> \n",
    "### Q: How \"sensitive\" is the loss $\\mathcal{L}$ to the change in a weight? \n",
    "\n",
    "<table><tr>\n",
    "<td> \n",
    "  <p align=\"center\" style=\"padding: 10px\">\n",
    "    <img alt=\"Forwarding\" src=\"img/last_layer.png\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Last layer change</em>\n",
    "  </p> \n",
    "</td>\n",
    "<td> \n",
    "  <p align=\"center\">\n",
    "    <img alt=\"Routing\" src=\"img/first_layer.png\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">First layer change</em>\n",
    "  </p> \n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "## **The backpropagation equations:**  \n",
    "## \\begin{align}\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial W_{i, 1}}(y, t) =  \\overset{\\text{ Change in } \\mathcal{L} \\text { from } y}{\\overbrace{2(y-t)}} \\cdot \\underset{\\text{... in } y \\text{ from change in }W_{i, 1}}{\\underbrace{a_j}} \\\\\n",
    "\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial V_{i, j}}(y, t) =  \\overset{\\text{ Change in } \\mathcal{L} \\text { from } y}{\\overbrace{2(y-t)}} \\cdot \\underset{\\text{ .. in } y \\text { from change in }a_j}{\\underbrace{W_{j, 1}} } \\cdot  \\overset{\\text{ .. in } a_i \\text { from change in } z_j}{\\overbrace{a_j(1-a_j)}}\\cdot \\underset{\\text{... in } z_j \\text{ from change in }V_{i, j}}{\\underbrace{x_i}}\n",
    "\\end{align}\n",
    "\n",
    "### To change the weight $W_{j, 1}$ in the output layer we use: \n",
    "# \\begin{align} W_{j, 1} & \\leftarrow W_{j,1} - \\eta \\cdot \\underset{\\delta_o} {\\underbrace{2(y-t)}}\\cdot a_j  \\\\\n",
    "W_{j, 1} & \\leftarrow W_{j,1} -\\eta \\cdot \\delta_o \\cdot a_j\n",
    "\\end{align}\n",
    "\n",
    "### To change the weight $V_{i, j}$ in the first layer, we use \n",
    "# \\begin{align} V_{i, j} & \\leftarrow V_{i, j} - \\eta \\cdot \\underset{\\delta_{h(j)}}{\\underbrace{2(y-t) W_{j, 1} a_{j} (1-a_j)}} \\cdot x_i \\\\ \n",
    "V_{i, j} & \\leftarrow V_{i, j} - \\eta \\cdot \\delta_{h(j)} x_i \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Back to our example:** \n",
    "\n",
    "### We use a learning rate of $\\eta = 0.1$\n",
    "\n",
    "### Input value was $x = (1, 2)$, target $t = 5$\n",
    "\n",
    "### After the forward pass we found $y = -0.59$\n",
    "\n",
    "### Let's begin to calculate the ouput delta: \n",
    " ## \\begin{align}\n",
    " \\delta_{o} & = 2(y-t) \\\\  & = 2\\cdot(-5.59) \\\\ &= -11.18\n",
    " \\end{align}\n",
    " \n",
    "### We first train the weight connecting the last **bias** to the output neuron: \n",
    "\n",
    "![image.png](img/img_4.png)\n",
    "\n",
    "## \\begin{align} \n",
    "    W_{0, 1} &\\leftarrow W_{0.1} - \\eta \\cdot \\delta_{o} \\cdot \\overset{\\text{bias} = -1} { \\overbrace{a_{0}}} \\\\\n",
    "     \\eta \\cdot \\delta_{o} \\cdot a_0  &= 0.1 \\cdot -11.18 \\cdot -1 \\\\\n",
    "     &= 1.118 \\\\\n",
    "     \\Longrightarrow W_{0.1} & \\leftarrow 1 - 1.118\\\\\n",
    "     &= -0.118\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### We repeat this for output weights $W_{1, 1}, W_{2, 1} \\text{ and }W_ {3, 1}$\n",
    "![image.png](img/img_3.png)\n",
    "## \\begin{align}\n",
    "W_{1, 1} &\\leftarrow W_{1,1} - \\eta \\cdot \\delta_{o} \\cdot a_1 \\\\\n",
    "     \\eta \\cdot \\delta_{o} \\cdot a_1  &= 0.1 \\cdot -11.18 \\cdot 0.5 \\\\\n",
    "     &= -0.559\\\\\n",
    "     \\Longrightarrow W_{1,1} & \\leftarrow 2 - (-0.559) \\\\\n",
    "     &= 2.559\n",
    "\\end{align}\n",
    "<br> <br> \n",
    "## \\begin{align}\n",
    "W_{2, 1} &\\leftarrow W_{2,1} - \\eta \\cdot \\delta_{o} \\cdot a_2 \\\\\n",
    "     \\eta \\cdot \\delta_{o} \\cdot a_2  &= 0.1 \\cdot -11.18 \\cdot 0.119 \\\\\n",
    "     &= -0.133\\\\\n",
    "     \\Longrightarrow W_{1,1} & \\leftarrow 3 - (-0.133) \\\\\n",
    "     &= 3.133\n",
    "\\end{align}\n",
    "<br><br>\n",
    "## \\begin{align}\n",
    "W_{3, 1} &\\leftarrow W_{3,1} - \\eta \\cdot \\delta_{o} \\cdot a_3 \\\\\n",
    "     \\eta \\cdot \\delta_{o} \\cdot a_3  &= 0.1 \\cdot -11.18 \\cdot 0.952 \\\\\n",
    "     &= -1.064\\\\\n",
    "     \\Longrightarrow W_{1,1} & \\leftarrow -1 - (-1.064) \\\\\n",
    "     &= 0.064\n",
    "\\end{align}\n",
    "\n",
    "## We now know all the new weights in the output layer (But don't update them yet! We need them to calculte the hidden deltas):\n",
    "\n",
    "\n",
    "![image.png](img/img_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, the weights in the first layer. Recall: \n",
    "\n",
    "### \\begin{align} \\overset{\\text{from input neuron i into hidden neuron j}}{\\overbrace{V_{i, j}}} & \\leftarrow V_{i, j} - \\eta \\cdot \\underset{\\delta_{h(j) =\\text{ hidden delta no. j} }}{\\underbrace{2(y-t) W_{j, 1} a_{j} (1-a_j)}} \\cdot \\overset{\\text{input neuron no. i}}{ \\overbrace{x_i}} \\\\ \n",
    "V_{i, j} & \\leftarrow V_{i, j} - \\eta \\cdot \\delta_{h(j)} x_i \n",
    "\\end{align}\n",
    "\n",
    "### Q: How does adjusting a weight in the first layer change the loss? \n",
    "\n",
    "### <font color = \"red\"> Beware: There is an error in the calculatons below. I somehow calculated the output delta to $-13.8$, not $-11.18$. This means that the final numbers are not exactly what we wanted, but the medthod is the same. Just repace $-13.8$ with $-11.18$ </font>\n",
    "![first layer](img/first_layer.png)\n",
    "\n",
    "### Let's first calculate the **hidden deltas** \n",
    "\n",
    "![The network](img/backward_prop.png)\n",
    "### \\begin{align}\n",
    "\\delta_{h(1)} = 2(y-t) W_{1, 1} a_1 (1-a_1) &= -13.8 \\cdot 2 \\cdot 0.5 \\cdot (1-0.5)\\\\\n",
    "\\delta_{h(2)} = 2(y-t) W_{2, 1} a_2 (1-a_2) &= -13.8 \\cdot 3 \\cdot 0.119 \\cdot (1-0.119)\\\\\n",
    "\\delta_{h(3)} = 2(y-t) W_{3, 1} a_3 (1-a_3) &= -13.8 \\cdot -1 \\cdot 0.952 \\cdot (1-0.952)\\\\\n",
    "\\end{align}\n",
    "### This gives\n",
    "### \\begin{align}\n",
    "\\delta_{h(1)} &= -6.9 \\\\\n",
    "\\delta_{h(2)} &= -4.34 \\\\\n",
    "\\delta_{h(3)} &= 0.63\\\\\n",
    "\\end{align}\n",
    "\n",
    "### We can finally update the weights:\n",
    "\n",
    "### \\begin{align}\n",
    "\\quad V_{0, 1}  \\leftarrow V_{0, 1} - \\eta \\cdot \\delta_{h(1)}\\cdot \\overset{\\text{bias}}{\\overbrace{-1}} \\Longrightarrow V_{0, 1} &= 1 - 0.1 \\cdot -6.9 \\cdot -1 = 0.31\\\\\n",
    "V_{0, 2}  \\leftarrow V_{0, 2} - \\eta \\cdot \\delta_{h(2)}\\cdot -1 \\Longrightarrow V_{0, 2} &= 0 - 0.1 \\cdot -4.34 \\cdot -1 = -0.434\\\\\n",
    "V_{0, 3}  \\leftarrow V_{0, 3} - \\eta \\cdot \\delta_{h(3)}\\cdot -1 \\Longrightarrow V_{0, 3} &= -1 - 0.1 \\cdot 0.63 \\cdot -1 = -0.9370\n",
    "\\end{align}\n",
    "### \\begin{align}\n",
    "V_{1, 1}  \\leftarrow V_{1, 1} - \\eta \\cdot \\delta_{h(1)}\\cdot x_1 \\Longrightarrow V_{1, 1} &= 1 - 0.1 \\cdot -6.9 \\cdot 1 = 1.69\\\\\n",
    "V_{1, 2}  \\leftarrow V_{1, 2} - \\eta \\cdot \\delta_{h(2)}\\cdot x_1 \\Longrightarrow V_{1, 2} &= 2 - 0.1 \\cdot -4.34 \\cdot 1 = 2.434\\\\\n",
    "V_{1, 3}  \\leftarrow V_{1, 3} - \\eta \\cdot \\delta_{h(3)}\\cdot x_1 \\Longrightarrow V_{1, 3} &= 0  - 0.1 \\cdot 0.63 \\cdot 1 = -0.063\n",
    "\\end{align}\n",
    "### \\begin{align}\n",
    "V_{2, 1}  \\leftarrow V_{2, 1} - \\eta \\cdot \\delta_{h(1)}\\cdot x_2 \\Longrightarrow V_{2, 1} &= 0 - 0.1 \\cdot -6.9 \\cdot 2 = 1.38\\\\\n",
    "V_{2, 2}  \\leftarrow V_{2, 2} - \\eta \\cdot \\delta_{h(2)}\\cdot x_2 \\Longrightarrow V_{2, 2} &= -2 - 0.1 \\cdot -4.34 \\cdot 2 = -1.132\\\\\n",
    "V_{2, 3}  \\leftarrow V_{2, 3} - \\eta \\cdot \\delta_{h(3)}\\cdot x_2 \\Longrightarrow V_{2, 3} &= 1 - 0.1 \\cdot 0.63 \\cdot 2 = 0.874\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary of the mathematics**: \n",
    "## To change the weight $W_{j, 1}$ in the output layer we use: \n",
    "# \\begin{align} W_{j, 1} & \\leftarrow W_{j,1} - \\eta \\cdot \\underset{\\delta_o} {\\underbrace{2(y-t)}}\\cdot a_j  \\\\\n",
    "W_{j, 1} & \\leftarrow W_{j,1} -\\eta \\cdot \\delta_o \\cdot a_j\n",
    "\\end{align}\n",
    "![Last layer](img/last_layer.png)\n",
    "\n",
    "\n",
    "## To change the weight $V_{i, j}$ in the first layer, we use \n",
    "# \\begin{align} V_{i, j} & \\leftarrow V_{i, j} - \\eta \\cdot \\underset{\\delta_{h(j)}}{\\underbrace{2(y-t) W_{j, 1} a_{j} (1-a_j)}} \\cdot x_i \\\\ \n",
    "V_{i, j} & \\leftarrow V_{i, j} - \\eta \\cdot \\delta_{h(j)} x_i \n",
    "\\end{align}\n",
    "![first layer](img/first_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br> \n",
    "\n",
    "# **Mathematics of backpropagation**\n",
    "### The following will be the mathematics of finding the gradient of the loss function. \n",
    "### We consider training on a single datapoint. This is known as **stochastic gradient descent**. \n",
    "\n",
    "## Step 1: Find the partial derivatives of the loss function with respect to the weights in the last layer. \n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{j, 1}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{ \\partial W_{j, 1}} $$ \n",
    "\n",
    "### We have that $\\frac{ \\partial{\\mathcal{L}} }{ \\partial y }(y, t) = 2(y-t)$\n",
    "\n",
    "### $y = -1 \\cdot W_{0, 1} + a_1 \\cdot W_{1, 1} + a_2 \\cdot W_{2, 1} + \\dots $, so $\\frac{\\partial y}{\\partial W_{j, 1}} = a_j $.\n",
    "### This means that we can update weight $W_{j, 1}$ in the **last layer** using \n",
    "\n",
    "### $$W_{j, 1} \\leftarrow W_{j, 1} -  \\eta 2  (y-t)a_j$$\n",
    "\n",
    "## Step 2: How about the first layer? \n",
    "### \\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial V_{i, j}} &= \\frac{\\partial \\mathcal{L}}{\\partial V_{i, j}} \\frac{\\partial y}{ \\partial V_{i, j}} \\\\\n",
    "&= 2(y-t) \\frac{\\partial y}{ \\partial V_{i, j}}\n",
    "\\end{align}\n",
    "### What is $ \\frac{\\partial y}{ \\partial V_{i, j}}$? \n",
    "### Have that \n",
    "### \\begin{align}\n",
    "y = &-1 \\cdot W_{0, 1} + \\sigma(z_1)W_{1, 1} + \\sigma(z_2) W_{2, 1} + \\sigma(z_3) W_{3, 1}\\\\\n",
    " &= -1 \\cdot W_{0, 1} + a_1 W_{1, 1} + a_2 W_{2, 1} + a_3 W_{3, 1},\n",
    "\\end{align}\n",
    "### so\n",
    "### $$ \\frac{\\partial y}{ \\partial V_{i, j}} =  \\frac{\\partial y}{ \\partial a_j} \\frac{\\partial a_j}{ \\partial V_{i, j}},$$ since changing weight $V_{i, j}$ only changes the value of $a_j$. (All the other terms involving $a_k, k \\neq j$ will become zero)  and $\\frac{\\partial y}{ \\partial a_j} = W_{j, 1}$.\n",
    "\n",
    "### Moreover,  $a_i = \\sigma(z_i).$ The derivative of the sigmoid function $a_i = \\sigma(z_i)$ is simply $a_i(1-a_i)$, so\n",
    "### $\\frac{\\partial a_j}{ \\partial V_{i, j}} = a_j(1-a_j) \\frac{\\partial z_j}{\\partial V_{i, j}}. $\n",
    "### Finally, $z_j = \\sum_{k} x_k V_{k, j}$, so $\\frac{\\partial z_j}{\\partial V_{i, j}} = x_i$. \n",
    "\n",
    "## Altogether: \n",
    "## \\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial V_{i, j}}(y, t) &= \\left(\\frac{\\partial \\mathcal{L}}{\\partial y}(y, t)\\right)\\frac{\\partial y}{\\partial a_j}\\frac{\\partial a_j}{z_j}\\frac{\\partial z_j}{\\partial V_{i, j}}\\\\ \n",
    "&= 2(y-t)W_{j, 1}a_j(1-a_j)x_i\n",
    "\\end{align}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
